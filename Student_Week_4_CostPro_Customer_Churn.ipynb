{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGc8tYh3Tu2s"
      },
      "source": [
        "# CostPro Customer Churn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJVztQB2Tu2t"
      },
      "source": [
        "🚨 **First things first! Make a copy of this notebook. Your changes will not save unless you create your own copy!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arYfVhbHTu2u"
      },
      "source": [
        "## 💡 Build Intuition\n",
        "\n",
        "As a junior data scientist at CostPro, you are approaching the problem of customer churn using non-parametric tests. These tests are useful because they do not make assumptions about the underlying distribution of the data, which is important in this case as the data may not meet the assumptions of parametric tests. For example, tests like chi-squared test or the Mann-Whitney U Test analyze the relationship between different features and customer churn. By using non-parametric tests you can have a deeper understanding of the factors that contribute to customer churn at CostPro and develop strategies to mitigate it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsGeUbOnTu2u"
      },
      "source": [
        "## 🚀 Project Jumpstart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H98qrRHKTu2u"
      },
      "source": [
        "### Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvyZeTHATu2u"
      },
      "outputs": [],
      "source": [
        "!pip install -qqq numpy pandas seaborn matplotlib gdown scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLt9NFD2Tu2v"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as st\n",
        "from typing import List, Tuple, Dict, Callable, Optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZ7gPAtmTu2w"
      },
      "outputs": [],
      "source": [
        "# set the random seed\n",
        "random_seed = 43"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6cdUJncTu2w"
      },
      "source": [
        "### 💾  Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bblSQS-NTu2w"
      },
      "source": [
        "**💡** Build Intuition: Be sure to check out the [data dictionary!](https://docs.google.com/spreadsheets/d/1qT_DIq7Brs3t-sUgFSvV8ZU37cAKSlIbAAgCcmxCd3w/edit?usp=sharing) It will help you build intuition about what data is available to you and how you might want to use it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ncwtqt2Tu2w"
      },
      "source": [
        "#### Download the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Go to the shared link for the data, download it to your local machine, then upload it into colab via the files button on the left hand side. We appreciate your patience. There's an issue with file formatting when the file is imported with gdown.\n",
        "\n",
        "https://docs.google.com/spreadsheets/d/12vt6qCUZ8C_YWHnCQ6HR9iSXiLAOHWqNlBOOaBT9hqw/edit?usp=sharing"
      ],
      "metadata": {
        "id": "q22TcevuvgdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save the file as churn_data.csv or change the name here!\n",
        "file_name = 'churn_data.csv'"
      ],
      "metadata": {
        "id": "2B1sIiFPj8cG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hw1_H1N2Tu2x"
      },
      "outputs": [],
      "source": [
        "# import data and show first 5 rows\n",
        "data = pd.read_csv(file_name)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypfLAib-Tu2x"
      },
      "source": [
        "#### Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmnkkqqbTu2x"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfwkFIM7Tu2x"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MD1KTfBHTu2x"
      },
      "outputs": [],
      "source": [
        "data.describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUiPiCeeTu2x"
      },
      "outputs": [],
      "source": [
        "# check for null values\n",
        "data.isnull().sum(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qLJOw0-Tu2y"
      },
      "outputs": [],
      "source": [
        "# drop the null values\n",
        "data.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfZeiDm9Tu2y"
      },
      "outputs": [],
      "source": [
        "# check for null values again (optionally: use an assert statemetn to check for no nulls)\n",
        "data.isnull().sum(axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJpyJhHRTu2y"
      },
      "source": [
        "### Does a Client Using a Coupon Have a Relationship with Churn?\n",
        "\n",
        "One common strategy in retail is to provide a customer with a coupon in the hope of increasing the likelihood they will return to make another purchase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "at8GXydOTu2y"
      },
      "source": [
        "### ⚙️ Develop a Hypothesis\n",
        "\n",
        "💡 Build Intuition: [Review the relevant course material on hypothesis formulation and testing.](https://uplimit.com/course/applied-statistics-for-data-science/v2/module/hypothesis-testing-9mlrk)\n",
        "\n",
        "Now that we understand a little bit about our data, we know we're getting a binary or 1 / 0 answer when it comes to churn.\n",
        "\n",
        "Let's set up our null and alternative hypotheses about the impact of coupons on CostPro Customer Churn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHFY-vjbTu2y"
      },
      "source": [
        "##### Null Hypothesis ($H_0$)\n",
        "\n",
        "Offering clients a coupon use does not impact customer churn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ki6xgeU7Tu2y"
      },
      "source": [
        "##### Alternative Hypothesis ($H_1$)\n",
        "\n",
        "Offering clients a coupon does impact customer churn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RomZiC8RTu2z"
      },
      "outputs": [],
      "source": [
        "# make the coupon used column easier to work with\n",
        "data['NumberOfCouponsUsed'] = data['CouponUsed']\n",
        "data['CouponUsed'] = data['CouponUsed'].apply(lambda x: 1 if x > 0 else 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7ueSgoETu2z"
      },
      "outputs": [],
      "source": [
        "# review the coupon used column to make sure it worked\n",
        "coupons = data['CouponUsed'].unique().tolist()\n",
        "coupons.sort()\n",
        "coupons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHPnRtoxTu2z"
      },
      "outputs": [],
      "source": [
        "# check the value counts for the coupon used column\n",
        "data['CouponUsed'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-obkMYcTu2z"
      },
      "outputs": [],
      "source": [
        "# Write a function to return two samples of the same size\n",
        "def get_samples(\n",
        "    data: pd.DataFrame, sample_size: int, independent_variable: str, dependent_variable: str\n",
        ") -> tuple:\n",
        "    \"\"\"\n",
        "    Returns two samples of the same size from the data.\n",
        "    data (pd.DataFrame): the data to be used\n",
        "    sample_size (int): the size of the sample to be returned\n",
        "    independent_variable (str): the name of the column to be used as the independent variable\n",
        "    dependent_variable (str): the name of the column to be used for the dependent_variable\n",
        "\n",
        "    Returns:\n",
        "    list: two samples of the same size\n",
        "    \"\"\"\n",
        "    independent_variable_list = data[independent_variable].unique().tolist()\n",
        "    independent_variable_list.sort()\n",
        "    print(f\"the independent variable list is {independent_variable_list}\")\n",
        "\n",
        "    samples = []\n",
        "\n",
        "    for i, var in enumerate(independent_variable_list):\n",
        "      sample = data[data[independent_variable] == independent_variable_list[i]].sample(\n",
        "          n=sample_size, random_state=random_seed\n",
        "      )[dependent_variable]\n",
        "      samples.append(sample)\n",
        "    return samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWmV90BMTu2z"
      },
      "source": [
        "## 🚧 Understand Limitations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vzq3DDUfTu2z"
      },
      "source": [
        "##### Normality\n",
        "\n",
        "In the case of non-parametric tests, we do not need the population to have a normal distribution. The reason why we're still checking whether or not the samples meet the assumptions of parametric tests is because parametric are more powerful. So, we want to be certain we need non-parametric tests before we select them.\n",
        "\n",
        "$H_0$: The sample comes from a normally distributed population.\n",
        "\n",
        "$H_1$: The sample does not come from a normally distributed population.\n",
        "\n",
        "How to interpret this test:\n",
        "\n",
        "If the p-value is less than 0.05, we reject the null hypothesis, suggesting that the sample is unlikely to have come from a normally distributed population."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwLZnh9HTu20"
      },
      "outputs": [],
      "source": [
        "# Write a function to test the assumption of normality\n",
        "def test_normality(samples: list) -> str:\n",
        "    \"\"\"\n",
        "    Tests the assumption of normality.\n",
        "    samples (list): the list of samples to be used\n",
        "\n",
        "    Returns:\n",
        "    None: prints the result of the test\n",
        "    \"\"\"\n",
        "    result = []\n",
        "\n",
        "    for i, sample in enumerate(samples):\n",
        "        if len(sample) > 2:\n",
        "            stat, p = st.shapiro(sample)\n",
        "            result.append((i, stat, p))\n",
        "            print(\"Shapiro test statistic:\", stat)\n",
        "            print(\"Shapiro p-value:\", p)\n",
        "            if round(p, 2) > 0.05:\n",
        "                print(\"The samples are normally distributed.\")\n",
        "            else:\n",
        "                print(\"The samples are not normally distributed.\")\n",
        "        else:\n",
        "            stat, critical_value, p = st.anderson_ksamp([sample, st.bernoulli.rvs(p=sample.mean(), size=sample.shape[0])])\n",
        "            result.append((i, stat, critical_value, p))\n",
        "            print('Anderson-Darling statistic:', stat)\n",
        "            print('Anderson-Darling p-value:', p)\n",
        "            if round(p, 2) > 0.05:\n",
        "                print(\"The samples are normally distributed.\")\n",
        "            else:\n",
        "                print(\"The samples are not normally distributed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrvWuJT5Tu20"
      },
      "outputs": [],
      "source": [
        "samples = get_samples(data, 500, \"CouponUsed\", \"Churn\")\n",
        "len(samples[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0dAR6eJTu20"
      },
      "outputs": [],
      "source": [
        "# Test the assumption of normality\n",
        "test_normality(samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain how you'd interpret the results of this test in your own words!\n",
        "\n",
        "Your explanation here."
      ],
      "metadata": {
        "id": "OfGeHVv_Xiqx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRM9BxQrTu21"
      },
      "source": [
        "#### Equal Variances\n",
        "\n",
        "One of the other assumptions underlying parametric tests is that the groups being compared have equal variances. This is called homoscedasticity. We can test for equal variances using a test called Levene's test.\n",
        "\n",
        "$H_0$: The samples have equal variances\n",
        "\n",
        "$H_1$: The samples do not have equal variances\n",
        "\n",
        "How to interpret this test:\n",
        "\n",
        "If the p-value is less that 0.05, we reject null hypothesis and say that the differences in sample variances are unlikely to have come from random sampling a population with equal variances.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kA9xeCfkTu21"
      },
      "outputs": [],
      "source": [
        "def test_homoscedasticity_bartlett(samples: list) -> str:\n",
        "    \"\"\"\n",
        "    Tests that both groups have equal variances using Bartlett's test.\n",
        "    samples (list): the list of samples to be used\n",
        "\n",
        "    Returns:\n",
        "    str: the result of the test\n",
        "    \"\"\"\n",
        "\n",
        "    stat, p = st.bartlett(*samples)\n",
        "    print(\"Bartlett test statistic:\", stat)\n",
        "    print(\"Bartlett p-value:\", p)\n",
        "    if p > 0.05:\n",
        "        return f\"The samples have equal variances with a Bartlett statistic of {stat} and a p-value of {p}\"\n",
        "    else:\n",
        "        return f\"The samples do not have equal variances with a Bartlett statistic of {stat} and a p-value of {p}\"\n",
        "\n",
        "\n",
        "def test_homoscedasticity_levene(samples: list) -> str:\n",
        "    \"\"\"\n",
        "    Tests that both groups have equal variances using Levene's test.\n",
        "    samples (list): the list of samples to be used\n",
        "\n",
        "    Returns:\n",
        "    str: the result of the test\n",
        "    \"\"\"\n",
        "\n",
        "    stat, p = st.levene(*samples)\n",
        "    print(\"Levene test statistic:\", stat)\n",
        "    print(\"Levene p-value:\", p)\n",
        "    if p > 0.05:\n",
        "        return f\"The samples have equal variances with a Levene statistic of {stat} and a p-value of {p}\"\n",
        "    else:\n",
        "        return f\"The samples do not have equal variances with a Levene statistic of {stat} and a p-value of {p}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hcw3-TAdTu21"
      },
      "outputs": [],
      "source": [
        "print(test_homoscedasticity_bartlett(samples))\n",
        "print()\n",
        "print(test_homoscedasticity_levene(samples))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE: visualize two samples and use the visualization to help you explain the results\n"
      ],
      "metadata": {
        "id": "qa8OhYt2rE6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain how you'd interpret the results of this test in your own words!\n",
        "\n",
        "Your explanation here."
      ],
      "metadata": {
        "id": "udYUWunGXcdv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elaFgoASTu21"
      },
      "source": [
        "## ⚙️ Implement the Chi-Squared Test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataframe from samples\n",
        "no_coupon_use = pd.DataFrame(samples[0], columns=['Churn'])\n",
        "no_coupon_use['CouponUsed'] = 0\n",
        "\n",
        "coupon_use = pd.DataFrame(samples[1], columns=['Churn'])\n",
        "coupon_use['CouponUsed'] = 1\n",
        "\n",
        "coupon_df = pd.concat([no_coupon_use, coupon_use])"
      ],
      "metadata": {
        "id": "mhksrkok7Ikk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5x2oTnKXTu21"
      },
      "outputs": [],
      "source": [
        "# set up the frequency table\n",
        "freq_table = pd.crosstab(coupon_df['CouponUsed'], coupon_df['Churn'])\n",
        "freq_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpAoW44VTu22"
      },
      "outputs": [],
      "source": [
        "# put the data into a list to be used in the chi-squared test\n",
        "observed = freq_table.values\n",
        "observed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ysjy7ocpTu22"
      },
      "outputs": [],
      "source": [
        "def test_hypothesis_with_chi_squared(observed: list) -> str:\n",
        "    \"\"\"\n",
        "    Tests the null hypothesis that the coupon use and churn are independent.\n",
        "    observed (list): the observed values\n",
        "\n",
        "    Returns:\n",
        "    str: the result of the test\n",
        "    \"\"\"\n",
        "    chi2, p, dof, expected = st.chi2_contingency(observed)\n",
        "    print(\"Chi-squared test statistic:\", chi2)\n",
        "    print(\"Chi-squared p-value:\", p)\n",
        "    if round(p, 2) <= 0.05:\n",
        "        print(\"The null hypothesis that the coupon use and churn are independent is rejected.\")\n",
        "        print(\"There is a relationship between coupon use and churn.\")\n",
        "    else:\n",
        "        print(\"The null hypothesis that the coupon use and churn are independent is accepted.\")\n",
        "        print(\"There is no relationship between coupon use and churn.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UNssopkTu22"
      },
      "source": [
        "Chi-squared test is used to determine if there is a significant association between two categorical variables, like coupon use and churn.\n",
        "\n",
        "To interpret the results of a chi-squared test, follow these steps:\n",
        "\n",
        "**Calculate the test statistic:** The test statistic is calculated by subtracting the expected frequencies from the observed frequencies, squaring the result, and dividing by the expected frequencies.\n",
        "\n",
        "**Determine the degrees of freedom:** The degrees of freedom for a chi-squared test are equal to the number of categories minus 1.\n",
        "\n",
        "**Look up the critical value:** Use a chi-squared distribution table to find the critical value for a given level of significance (e.g. 0.05) and degrees of freedom.\n",
        "\n",
        "**Compare the test statistic to the critical value:** If the calculated test statistic is greater than the critical value, the null hypothesis is rejected and there is evidence of a significant association between the two categorical variables.\n",
        "\n",
        "**Report the results:** Report the calculated test statistic, degrees of freedom, critical value, and level of significance. State if the null hypothesis is accepted or rejected and if there is evidence of a significant association between the two categorical variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZAW7zELTu22"
      },
      "outputs": [],
      "source": [
        "# calculate degrees of freedom\n",
        "def degrees_of_freedom(categories1, categories2):\n",
        "    degrees_of_freedom = (categories1 - 1) * (categories2 - 1)\n",
        "    return degrees_of_freedom\n",
        "\n",
        "df = degrees_of_freedom(len(data['CouponUsed'].unique()), len(data['Churn'].unique()))\n",
        "print(\"Degrees of freedom:\", df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uba7f8tuTu22"
      },
      "source": [
        "### Calculate the Critical Value\n",
        "\n",
        "PPF stands for \"percent point function\". In statistics, the PPF is also known as the inverse cumulative distribution function (CDF). The CDF of a random variable gives the probability that the variable is less than or equal to a given value, while the PPF gives the value at which the CDF equals a given probability.\n",
        "\n",
        "In the context of the scipy library in Python, the chi2.ppf function is used to get the critical value for a chi-squared test by finding the value at which the cumulative distribution function (CDF) of the chi-squared distribution equals 1 - alpha. In other words, it finds the value at which the CDF equals alpha confidence level."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VivUjVOyTu22"
      },
      "outputs": [],
      "source": [
        "# calculate the critical value\n",
        "from scipy.stats import chi2\n",
        "\n",
        "def get_critical_value(alpha, degrees_of_freedom):\n",
        "    critical_value = chi2.ppf(1 - alpha, degrees_of_freedom)\n",
        "    return critical_value\n",
        "\n",
        "# Example usage\n",
        "alpha = 0.05\n",
        "critical_value = get_critical_value(alpha, degrees_of_freedom(len(data['CouponUsed'].unique()), len(data['Churn'].unique())))\n",
        "print(\"Critical value:\", critical_value)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FP8kYKtTu23"
      },
      "source": [
        "### Review and Interpret the Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZZ9FgnrTu23"
      },
      "outputs": [],
      "source": [
        "test_hypothesis_with_chi_squared(observed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybb2H3jJTu23"
      },
      "source": [
        "In the case where the p-value is exactly 0.05, some practitioners would reject the null hypothesis, while others would fail to reject it. It depends on the researcher's tolerance for Type I errors (rejecting the null hypothesis when it is true) and Type II errors (failing to reject the null hypothesis when it is false). It will also depend on the specific context of your hypothesis test; sometimes the consequences of a Type II error will be worse than a Type I error, but in other cases a Type I error would be worse than a Type II error.\n",
        "\n",
        "It's important to use a consistent and well-justified approach when dealing with p-values that are exactly equal to the significance level. In some cases, it may be advisable to set a more stringent significance level, such as 0.01 or 0.001, to reduce the risk of making a Type I error. It is important to consider your stakeholders' needs, and the risks of each type of error, when deciding what significance level is most appropriate for your hypothesis test.\n",
        "\n",
        "What significance level is appropriate, and what you should do when your p-value is exactly at your significance level, will depend on the relative risks of making a Type I error vs. a Type II error. If your null hypothesis is that X product is not making customers sick, a Type II error (failing to reject the null hypothesis when the product really is getting people sick) might be much worse than a Type I error, and so a higher significance level.\n",
        "\n",
        "Let's try exploring another question!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5Z2hjifTu23"
      },
      "source": [
        "### Does a Client Satisfaction Score Have a Relationship with Churn?\n",
        "\n",
        "Ideally, customer satisfaction is a leading indicator of customer churn. Let's test the hypothesis that there is a relationship between customer satisfaction and customer churn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtUBOZSxTu23"
      },
      "outputs": [],
      "source": [
        "data['SatisfactionScore'].hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNQ-u5GbTu23"
      },
      "outputs": [],
      "source": [
        "data['Churn'].hist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9o_Om8aTTu23"
      },
      "source": [
        "### ⚙️ Develop a Hypothesis\n",
        "\n",
        "💡 Build Intuition: [Review the relevant course material on hypothesis formulation and testing.](https://uplimit.com/course/applied-statistics-for-data-science/v2/module/hypothesis-testing-9mlrk)\n",
        "\n",
        "Let's set up our null and alternative hypotheses about the relationship between satisfaction score and customer churn.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M56TjX3UTu24"
      },
      "source": [
        "##### Null Hypothesis ($H_0$)\n",
        "\n",
        "The median satisfaction scores for customers who have churned and customers who haven't are the same or almost the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6qV_xWpTu24"
      },
      "source": [
        "##### Alternative Hypothesis ($H_1$)\n",
        "\n",
        "The median satisfaction scores for customers who have churned and customers who haven't are not the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-ehDSceTu24"
      },
      "outputs": [],
      "source": [
        "# Write a function to return two samples of the same size\n",
        "def get_two_samples(\n",
        "    data: pd.DataFrame, sample_size: int, treatment_column: str, outcome_column: str\n",
        ") -> tuple:\n",
        "    \"\"\"\n",
        "    Returns two samples of the same size from the data.\n",
        "    data (pd.DataFrame): the data to be used\n",
        "    sample_size (int): the size of the sample to be returned\n",
        "    outcome_column (str): the name of the column to be used for the outcome\n",
        "\n",
        "    Returns:\n",
        "    list: two samples of the same size\n",
        "    \"\"\"\n",
        "    outcomes = data[outcome_column].unique().tolist()\n",
        "    outcomes.sort()\n",
        "\n",
        "    sample_1 = data[data[outcome_column] == outcomes[0]].sample(\n",
        "        n=sample_size, random_state=random_seed\n",
        "    )[treatment_column]\n",
        "\n",
        "    sample_2 = data[data[outcome_column] == outcomes[1]].sample(\n",
        "        n=sample_size, random_state=random_seed\n",
        "    )[treatment_column]\n",
        "\n",
        "    return [sample_1, sample_2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIwewNDsTu24"
      },
      "outputs": [],
      "source": [
        "# write a function to test the null hypothesis that the two groups have the same median with a mann whitney u test\n",
        "def test_hypothesis_with_mann_whitney_u(samples: list) -> str:\n",
        "    \"\"\"\n",
        "    Tests the null hypothesis that the two groups have the same median.\n",
        "    samples (list): the list of samples to be used\n",
        "\n",
        "    Returns:\n",
        "    str: the result of the test\n",
        "    \"\"\"\n",
        "    u_statistic, p_value = st.mannwhitneyu(*samples)\n",
        "    print(\"Mann-Whitney U test statistic:\", u_statistic)\n",
        "    print(\"Mann-Whitney U p-value:\", p_value)\n",
        "    if round(p_value, 2) <= 0.05:\n",
        "        print(\"The null hypothesis that the two groups have the same median is rejected.\")\n",
        "        print(\"There is a relationship between satisfaction score and churn.\")\n",
        "    else:\n",
        "        print(\"The null hypothesis that the two groups have the same median is accepted.\")\n",
        "        print(\"There is no detectable relationship between satisfaction score and churn.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXU3xVZaTu24"
      },
      "source": [
        "To interpret the results of a Mann-Whitney U test, you need to compare the U statistic to the critical value. The critical value is calculated based on the sample sizes and the level of significance (e.g., alpha = 0.05). If the U statistic is less than the critical value, it suggests that the two samples come from populations with different medians and you reject the null hypothesis that the populations have equal medians. Conversely, if the U statistic is greater than or equal to the critical value, it suggests that the two samples come from populations with similar medians and you fail to reject the null hypothesis.\n",
        "\n",
        "In addition to the U statistic and the critical value, you can also calculate a p-value for the Mann-Whitney U test. The p-value is the probability of observing a U statistic as extreme or more extreme than the observed U statistic, given that the null hypothesis is true. If the p-value is less than the level of significance, you reject the null hypothesis. If the p-value is greater than or equal to the level of significance, you fail to reject the null hypothesis.\n",
        "\n",
        "It is important to keep in mind that the Mann-Whitney U test is a two-tailed test, which means that it tests for differences in either direction (i.e., one sample has a higher median or one sample has a lower median). If you are interested in testing for a specific direction of difference (e.g., one sample has a higher median), you need to adjust the level of significance or use a one-tailed test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfPI63KkTu24"
      },
      "outputs": [],
      "source": [
        "test_hypothesis_with_mann_whitney_u(\n",
        "    get_two_samples(data, 500, \"SatisfactionScore\" , \"Churn\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWepvRCkTu24"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zu4173h0Tu24"
      },
      "source": [
        "### Does a Preferred Device Have a Relationship with DaySinceLastOrder?\n",
        "\n",
        "While CostPro tries to maintain a high quality customer experience across the board, it might be possible that certain methods of interfacing with CostPro have a relationship with days since last order. This is another way to get at the problem of churn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2SiconOTu25"
      },
      "outputs": [],
      "source": [
        "data['PreferredLoginDevice'].hist();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11hvqkPMTu25"
      },
      "outputs": [],
      "source": [
        "sns.boxplot(data=data, x='DaySinceLastOrder', y='PreferredLoginDevice');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tPHIgngTu25"
      },
      "source": [
        "##### Null Hypothesis ($H_0$)\n",
        "\n",
        "There is no relationship between preferred login device and churn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ku69BqgTu25"
      },
      "source": [
        "##### Alternative Hypothesis ($H_1$)\n",
        "\n",
        "There is a relationship between preferred login device and churn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rI_0FIgRTu25"
      },
      "outputs": [],
      "source": [
        "samples = get_samples(data, 500, 'PreferredLoginDevice', 'DaySinceLastOrder')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HCsqC0gTu25"
      },
      "outputs": [],
      "source": [
        "# write a function to test the null hypothesis that n groups have the same mean using the kruskal wallis test\n",
        "def test_hypothesis_with_kruskal_wallis(samples: list) -> str:\n",
        "    \"\"\"\n",
        "    Tests the null hypothesis that n groups have the same mean.\n",
        "    samples (list): the list of samples to be used\n",
        "\n",
        "    Returns:\n",
        "    str: the result of the test\n",
        "    \"\"\"\n",
        "    stat, p_value = st.kruskal(*samples)\n",
        "    print(\"Kruskal-Wallis test statistic:\", stat)\n",
        "    print(\"Kruskal-Wallis p-value:\", p_value)\n",
        "    if round(p_value, 2) <= 0.05:\n",
        "        print(\"The null hypothesis that n groups have the same median is rejected.\")\n",
        "        print(\"There is a relationship between preferred login device and days since last order.\")\n",
        "    else:\n",
        "        print(\"The null hypothesis that n groups have the same mean is accepted.\")\n",
        "        print(\"There is no significant relationship between preferred login device and days since last order.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by1EWetXTu25"
      },
      "source": [
        "To interpret the results of a Kruskal-Wallis test, you will need to determine the p-value. The p-value is the probability of obtaining a test statistic as extreme or more extreme than the observed test statistic, under the null hypothesis. A small p-value indicates that the difference between the medians is statistically significant and that you should reject the null hypothesis.\n",
        "\n",
        "If the p-value is less than your chosen level of significance, such as 0.05, you can reject the null hypothesis and conclude that there is evidence of a difference in medians between at least two of the groups.\n",
        "\n",
        "It is important to keep in mind that the Kruskal-Wallis test provides a test of the overall difference between the medians of the groups, but it does not tell you which groups are different or how they are different. To further explore the differences between the groups, you may want to perform post-hoc tests.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvN-awriTu26"
      },
      "outputs": [],
      "source": [
        "test_hypothesis_with_kruskal_wallis(samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Interpret the Result\n",
        "\n",
        "Practice explaining this result to a business stakeholder by writing your interpretation. Remember to interpret the test in a way that is appropriate for your audience. If there's a helpful data visualization, please include it.\n",
        "\n",
        "YOUR WORDS HERE\n"
      ],
      "metadata": {
        "id": "BWf5bJAqoAQP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOyMpoopTu26"
      },
      "source": [
        "## Your Turn\n",
        "\n",
        "Develop and test hypotheses!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8NCYHn1Tu26"
      },
      "source": [
        "### ⚙️ Develop a Hypothesis\n",
        "\n",
        "💡 Build Intuition: [Review the relevant course material on hypothesis formulation and testing.](https://uplimit.com/course/applied-statistics-for-data-science/v2/module/hypothesis-testing-9mlrk)\n",
        "\n",
        "Try formulating a hypothesis that you can test with a chi-squared test.\n",
        "\n",
        "Remember that chi-squared is a way to test categorical variables in a contingency table.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kr4ncTz6Tu26"
      },
      "source": [
        "##### Null Hypothesis ($H_0$)\n",
        "\n",
        "<WRITE YOUR HYPOTHESIS HERE>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9zNri3zTu26"
      },
      "source": [
        "##### Alternative Hypothesis ($H_1$)\n",
        "\n",
        "<WRITE YOUR HYPOTHESIS HERE>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FvnUNuJTu26"
      },
      "outputs": [],
      "source": [
        "# get the samples\n",
        "# your_samples_1 = get_samples(data, 500, '<independent_variable>', '<dependent_variable>')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kr6RlcxTu26"
      },
      "outputs": [],
      "source": [
        "# display the cross tabulation\n",
        "# pd.crosstab(data['<independent_variable>'], data['<dependent_variable>'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WvUZrJ0Tu26"
      },
      "outputs": [],
      "source": [
        "# set up the observed values for the chi-squared test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0Nxm4zATu26"
      },
      "source": [
        "Run a chi-squared test and report the results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8cAVgBZTu27"
      },
      "outputs": [],
      "source": [
        "# run the test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDPTFf7OTu27"
      },
      "outputs": [],
      "source": [
        "# visualize the results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the results"
      ],
      "metadata": {
        "id": "Upnboe-Vn0AK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DT8sx0hNTu27"
      },
      "source": [
        "#### ⚙️ Develop Your Hypothesis\n",
        "\n",
        "💡 Build Intuition: [Review the relevant course material on hypothesis formulation and testing.](https://uplimit.com/course/applied-statistics-for-data-science/v2/module/hypothesis-testing-9mlrk)\n",
        "\n",
        "Now, develop and test a hypothesis with a test of your own choosing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfTVElOsTu27"
      },
      "source": [
        "##### Null Hypothesis\n",
        "\n",
        "Write your null hypothesis here.\n",
        "\n",
        "$H_0$:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCNbezZ4Tu27"
      },
      "source": [
        "##### Alternative Hypothesis\n",
        "\n",
        "Write your alternative hypothesis here.\n",
        "\n",
        "$H_1$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVZNtt05Tu27"
      },
      "outputs": [],
      "source": [
        "# get the samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-_H1cdqTu27"
      },
      "outputs": [],
      "source": [
        "# define the test function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-bcMZFgTu27"
      },
      "outputs": [],
      "source": [
        "# run the test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6EcPdAUTu27"
      },
      "outputs": [],
      "source": [
        "# visualize the results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the result"
      ],
      "metadata": {
        "id": "5bNuQ8-yP74p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Interpret the Result\n",
        "\n",
        "Practice explaining this result to a business stakeholder by writing your interpretation. Remember to interpret the test in a way that is appropriate for your audience. If there's a helpful data visualization, please include it.\n",
        "\n",
        "YOUR WORDS HERE\n"
      ],
      "metadata": {
        "id": "7jeEnh0cooR-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UpGiESagoodm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}